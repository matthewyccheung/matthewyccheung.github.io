<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[Project] Signal Denoising using 1D Kalman Filters | Matt Y. Cheung</title>
<meta name="description" content="">


  <meta name="author" content="Matt Y. Cheung">
  
  <meta property="article:author" content="Matt Y. Cheung">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Matt Y. Cheung">
<meta property="og:title" content="[Project] Signal Denoising using 1D Kalman Filters">
<meta property="og:url" content="http://localhost:4000/2019/06/07/1D-Kalman-Denoising.html">


  <meta property="og:description" content="">







  <meta property="article:published_time" content="2019-06-07T00:00:00-05:00">






<link rel="canonical" href="http://localhost:4000/2019/06/07/1D-Kalman-Denoising.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Matt Y. Cheung Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/images/home-solid.png" alt=""></a>
        
        <a class="site-title" href="/">
          Matt Y. Cheung
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/Publications/">Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/CV/">CV</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/images/IMG_3972%202%20copy.jpg" alt="Matt Y. Cheung" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Matt Y. Cheung</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>I’m a PhD student in the Electrical &amp; Computer Engineering Department at Rice University working under Professor <a href="https://profiles.rice.edu/faculty/ashok-veeraraghavan">Ashok Veeraraghavan</a> in the <a href="http://computationalimaging.rice.edu/">Rice Computational Imaging Lab</a>. Broadly speaking, I’m interested solving real-world problems using fundamentals of Computational Imaging, DSP and Machine Learning.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Houston, TX</span>
        </li>
      

      
        
          
            <li><a href="https://github.com/matthewyccheung" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://scholar.google.com/citations?user=6d3hfUcAAAAJ&hl=en" rel="nofollow noopener noreferrer"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span class="label">Google Scholar</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/matthewyccheung/" rel="nofollow noopener noreferrer"><i class="fab fa-linkedin-in" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="mailto:mattyc@rice.edu" rel="nofollow noopener noreferrer"><i class="fas fa-at" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[Project] Signal Denoising using 1D Kalman Filters">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="2019-06-07T00:00:00-05:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[Project] Signal Denoising using 1D Kalman Filters
</h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        <time datetime="2019-06-07T00:00:00-05:00">June 7, 2019</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="introduction">Introduction</h1>

<p>The Kalman filter is an optimal estimator that infers parameters
uncertain and inaccurate measurements. The recursion implementation
implies that new measurements can be processed as they arrive. The
Kalman Filter uses this concept of perturbation to "update states"
without recursively solving a larger and larger least squares problem.
This is advantageous because solving increasingly larger least squares
problems is incredibly time consuming. This section pertains to the
concepts and derivation of the Kalman Filter. Firstly, perturbing
inverse of a matrix is explored through the work of Hager (1989). Next,
the update concept to derive the Kalman Filter formulae. This is a
modified version of the derivation presented by Lacey (1998).
Ultimately, the filter reduces to updating 5 equations.</p>

<h2 id="solving-overdetermined-systems-normal-equation">Solving Overdetermined Systems: Normal Equation</h2>

<p>An overdetermined system $Ax = b$ can be solved by minimizing
$| b - Ax |_2^2$ where $A \in {\rm I!R}^{m \times n}$, $m &gt; n$,
$x \in {\rm I!R}^{n \times 1}$ and $b \in {\rm I!R}^{m \times 1}$
<script type="math/tex">\| b - Ax \|^2_2 = x^T A^T A x - 2bAx + \|b\|^2_2</script> The overdetermined
system is equivalent to solving the square system <script type="math/tex">A^T A x = A^T b</script>
This is called the normal equation and the solution is
<script type="math/tex">x = (A^T A)^{-1} A^T b</script></p>

<h2 id="perturbing-any-invertible-matrix-a">Perturbing any Invertible Matrix A</h2>

<p>The inverse of an invertible matrix A perturbed by a rank k matrix
$UV^T$ is given by the Sherman-Morrison-Woodbury formula</p>

<script type="math/tex; mode=display">M^{-1} = (A - UV^T)^{-1} = A^{-1} + A^{-1}U(I-V^T A^{-1}U)^{-1} V^T A^{-1}</script>

<h2 id="updating-least-squares">Updating Least Squares</h2>

<p>Suppose we receive new data that has the relation $vx = c$. THe new
"A" matrix is given by <script type="math/tex">A_1 \\
=
\begin{bmatrix}
A_0 \\
v \\
\end{bmatrix}</script> Let $B = A^T A$. The updated B can be given the the
equation <script type="math/tex">% <![CDATA[
B_{k+1} = A_{k+1}^T A_{k+1} = 
\begin{bmatrix}
A_k^T & v^T \\
\end{bmatrix}
\begin{bmatrix}
A_k \\
v \\
\end{bmatrix}
= A_k^T A_k + v^T v = B_k + v^Tv %]]></script> Let $u = v^T$. From the
Sherman-Morrison-Woodbury formula, we can attain
<script type="math/tex">B_{k+1}^{-1} = (B_k + uv)^{-1} = B_k^{-1} - \alpha B_k^{-1}uvB_k^{-1}</script>
where <script type="math/tex">\alpha = \frac{1}{1 + vB_k^{-1} u}</script> This implies that the new B
can be found by adding a rank 1 correction to the first matrix. This
form can be used to produce the new least squares solution with the new
B <script type="math/tex">x_1 = x_0 + k(c-vx_0)</script> where <script type="math/tex">k = \frac{B_k^{-1}u}{1+vB_k^{-1}u}</script>
Similarly, if the new data has the relation $Vx = C$, then the new x can
be expressed by <script type="math/tex">x_1 = x_0 + K(C-Vx_0)</script> where
<script type="math/tex">K = B_k^{-1}U(I + VB_k^{-1}U)^{-1}</script> $B_{k+1}^{-1}$ can be expressed
as <script type="math/tex">B_{k+1}^{-1} = B_k^{-1} - B_k^{-1}U(I + VB_k^{-1}U)^{-1}VB_k^{-1}</script></p>

<h2 id="kalman-filter">Kalman Filter</h2>

<p>The Kalman Filter uses the updated least squares solution form
$x_1 = x_0 + k(c-vx_0)$ to attain a new estimate when new data arrives.
This implies that a new linear system does not need to be fully solved.</p>

<h3 id="state-vectors-measurement-vectors-and-covariances">State Vectors, Measurement Vectors and Covariances</h3>

<p>Suppose the true values are represented by the equation
<script type="math/tex">x_{k+1} = \Phi x_k + w_k</script> where $x\in {\rm I!R}^{m\times 1}$ is
called the state vector at time k, $\Phi \in {\rm I!R}^{m\times n}$ is
the transition matrix from time k to time k+1 (assumed to be time
invariant) and $v_k \in {\rm I!R}^{m\times 1}$ is the associated white
noise process. The observations can be represented by the equation
<script type="math/tex">z_k = Hx_k + v_k</script> where $z_k\in {\rm I!R}^{m\times 1}$ is the actual
measurement vector at time k, $H \in {\rm I!R}^{m\times n}$ is the
noiseless connection between state and measurement vector (assumed to be
time invariant) and $v_k \in {\rm I!R}^{m\times 1}$ is the associated
measurement vector (white noise process). The covariances of the noise
models mentioned above can be represented by <script type="math/tex">Q = E[w_k w_k^T]</script>
<script type="math/tex">R = E[v_k v_k^T]</script> where Q is the state covariance and R is the
measurement covariance. Similarly, the covariance of the error (mean
squared error) can be represented by <script type="math/tex">P_k = E[e_k e_k^T]</script> where
<script type="math/tex">e_k = x_k - \hat x_k</script></p>

<h3 id="update-equation-for-new-estimate">Update Equation for New Estimate</h3>

<p>The update equation from the new estimate can be put in the form
<script type="math/tex">\hat x_k = \hat x'_k + K_k (z_k - H\hat x'_k)</script> $x’_k$ is the prior
estimate of $x_k$. $z_k - H\hat x’_k$ is called the innovation $i_k$.
This is equivalent to the updated least squares solution
<script type="math/tex">x_k = x_{k-1} + K_k(C-Vx_{k-1})</script> In this case, C is z and V is H.</p>

<h3 id="relating-p_k-and-p_k-1">Relating $P_k$ and $P_{k-1}$</h3>

<p>In order to relate $P_{k+1}$ and $P_k$, substitute <script type="math/tex">z_k = Hx_k + v_k</script>
into <script type="math/tex">\hat x_k = \hat x_{k-1} + K_k (z_k - H\hat x_{k-1})</script> to attain
<script type="math/tex">\hat x_k = \hat x'_k + K_k (Hx_k + v_k - H\hat x'_k)</script></p>

<p>Therefore, the estimation error is
<script type="math/tex">e_k = x_k - \hat x_k = (I - K_k H)(x_k - \hat x'_k) - K_k v_k</script></p>

<p>And the covariance of the previous estimation error is</p>

<p><script type="math/tex">P_k = E[[(I - K_k H)(x_k - \hat x'_k) - K_k v_k][(I - K_k H)(x_k - \hat x'_k) - K_k v_k]^T]</script>
<script type="math/tex">= (I - K_k H)E[(x_k - \hat x'_k)(x_k - \hat x'_k)^T](I - K_k H)^T + K_kE[v_kv_k^T]K_k^T</script>
$E[(x_k - \hat x’_k)(x_k - \hat x’_k)^T]$ can be replaced by $P’_k$,
which is the error covariance for the previous estimate. $E[v_kv_k^T]$
can be replaced by R (from the above definition)</p>

<p>Therefore, <script type="math/tex">P_k = (I - K_k H)P'_k(I - K_k H)^T + K_kRK_k^T</script></p>

<p>An expansion of $P_k$ gives the form
<script type="math/tex">P_k = P'_k - K_k H P'_k - P'_k H^T K_k^T + K_k(HP'_kH^T + R) K_k^T</script></p>

<p>The covariance matrix form of $P_k$ can be represented in the form</p>

<script type="math/tex; mode=display">% <![CDATA[
P_k = 
\begin{bmatrix}
E[e_{k-1} e_{k-1}] & E[e_{k-1} e_k] & E[e_{k-1} e_{k+1}] \\
E[e_k e_{k-1}] & E[e_k e_k] & E[e_k e_{k+1}] \\
E[e_{k+1} e_{k-1}] & E[e_{k+1} e_k] & E[e_{k+1} e_{k+1}] \\
\end{bmatrix} %]]></script>

<p>Noting that $tr(K_k H P’_k) = tr(P’_k H^T K_k^T)$ because
$tr(A) = tr(A^T)$, the trace of the expansion is
<script type="math/tex">tr(P_k) = tr(P'_k) - 2tr(K_kHP'_k) + tr(K_k(HP'_kH^T + R)K_k^T)</script></p>

<p>From this, we can see that
<script type="math/tex">tr(P_k) = E[e_{k-1} e_{k-1}] + E[e_k e_k] + E[e_{k+1} e_{k+1}]</script></p>

<p>Minimizing the trace of $P_k$ minimizes the mean squared error</p>

<h3 id="kalman-gain">Kalman Gain</h3>

<p>To get the gain that minimizes the mean squared error, we must take
$\frac{dtr(P_k)}{dK_k}$
<script type="math/tex">\frac{dtr(P_k)}{dK_k} = -2(HP'_k)^T + 2K_k(HP'_kH^T + R) = 0</script>
<script type="math/tex">K_k = P'_kH^T(HP'_kH^T + R)^{-1}</script> The innovation $i_k$ covariance is
given by <script type="math/tex">S_k = HP'_kH^T + R</script></p>

<h3 id="solving-for-p_k">Solving for $P_k$</h3>

<p>Using the Kalman Gain equation, we can substitute it into the expanded
$P_k$
<script type="math/tex">P_k = P'_k - K_k H P'_k - P'_k H^T K_k^T + K_k(HP'_kH^T + R) K_k^T</script>
<script type="math/tex">P_k = (I-K_kH)P'_k</script></p>

<h3 id="estimated-state-projection-and-error-covariance">Estimated State Projection and Error Covariance</h3>

<p>The state projection is done using <script type="math/tex">\hat
x'_{k+1} = \Phi \hat x_k</script> The error covariance for the next time
interval is given by <script type="math/tex">e'_{k+1} = x_{k+1} - x'_{k+1} = \Phi e_k + w_k</script>
Finally, <script type="math/tex">P'_{k+1} = \Phi P_k \Phi^T + Q</script></p>

<h1 id="algorithm">Algorithm</h1>

<p>The update equations are as follows</p>

<p><script type="math/tex">K_k = P'_kH^T(HP'_kH^T + R)^{-1}</script>,</p>

<p><script type="math/tex">\hat x_k = \hat x_{k-1} + K_k (z_k - H\hat x_{k-1})</script>,</p>

<p><script type="math/tex">P_k = (I-K_kH)P'_k</script>,</p>

<p><script type="math/tex">x'_{k+1} = \Phi \hat x_k</script>,</p>

<p><script type="math/tex">P'_{k+1} = \Phi P_k \Phi^T + Q</script>,</p>

<p>The Kalman Filter Algorithm can be boiled down to a few lines of code</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">pi</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kf_1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">xhat</span><span class="p">,</span> <span class="n">xhat_prev</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">P_prev</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">xhat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'xhat0'</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'p0'</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">arr_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arr_size</span><span class="p">):</span>
        <span class="n">xhat_prev</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">phi</span><span class="o">*</span><span class="n">xhat</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">P_prev</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">phi</span><span class="o">*</span><span class="n">P</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">phi</span><span class="p">)</span> <span class="o">+</span> <span class="n">Q</span>
        <span class="n">K</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">P_prev</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">H</span><span class="o">/</span><span class="p">(</span><span class="n">H</span><span class="o">*</span><span class="n">P_prev</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">H</span><span class="p">)</span> <span class="o">+</span> <span class="n">R</span><span class="p">)</span>
        <span class="n">xhat</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">xhat_prev</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">K</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">xhat_prev</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">P</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">K</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">H</span><span class="p">)</span><span class="o">*</span><span class="n">P_prev</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">xhat</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_error</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xhat</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">abs</span><span class="p">(</span><span class="n">xhat</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">x</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># set truth values
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span><span class="o">*</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">arr_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">arr_size</span><span class="p">)</span>

<span class="c1"># initialize arrays
</span><span class="n">xhat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">arr_size</span><span class="p">)</span>
<span class="n">xhat_prev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">arr_size</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">arr_size</span><span class="p">)</span>
<span class="n">P_prev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">arr_size</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">arr_size</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="general-operation">General Operation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e-10</span><span class="p">])</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e-4</span><span class="p">])</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>

<span class="n">xhat</span> <span class="o">=</span> <span class="n">kf_1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">xhat</span><span class="p">,</span> <span class="n">xhat_prev</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">P_prev</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="methods">Methods</h1>

<p>A standard signal with value 1.0 and length 1000 were used to
investigate the Kalman Filter. After Gaussian noise was applied to the
signal, each signal is passed into the filter iteratively and an
estimate is computed for each point. The Gaussian noise is applied with
mean of 1 and standard deviation of 0.1</p>

<h2 id="kalman-filter-denoising">Kalman Filter Denoising</h2>

<p>A 1D signal and a 2D signal (image) is used to demonstrate the Kalman
Filter. The Q, R and H were arbitrarily chosen to be Q = 1e-10, R = 1e-4
and H = 1.0. The image used was the AT3_1m4_01.tif from the MATLAB
built in images and is shown below</p>

<p><img src="pics/AT3_1m4_01.png" alt="AT3_1m4_01.tif image from the MATLAB built in image
dataset" /></p>

<p>Each image for each iteration is given a new gaussian noise
distribution. The noise ratio is computer by adding a fraction (called
noise ratio, 1e0) of the maximum value of the image multiplied by a
gaussian distribution to the image. This is exemplified in the code
shown in the appendix.</p>

<h2 id="convergence">Convergence</h2>

<p>Different values for the initial guess (xhat[0] from 0 to 2), process
variances Q (1e-10, 1e-6, 1e-2) and measurement variances R (1e-10,
1e-6, 1e-2) were used to investigate the effect on mean squared error
convergence.</p>

<p>The Kalman Filter is incredibly useful for denoising signals, whether
the error is gaussian distributed, exponentially distributed or
uniformly distributed. These three noise models are used to investigate
convergence of the filter.</p>

<h1 id="numerical-results">Numerical Results</h1>

<h2 id="kalman-filter-denoising-1">Kalman Filter Denoising</h2>

<p>The true signals, measured signals and estimated signals are shown below</p>

<p><img src="pics/1dsignals.png" alt="Plot of true signals, measured signals and estimated
signals" /></p>

<p>From this plot, the Kalman Filter presents a good result for denoising
unchanging signals with Gaussian noise.</p>

<p><img src="pics/microscope_denoise.png" alt="AT3_1m4_01.tif image from the MATLAB built in image
dataset" /></p>

<h2 id="convergence-1">Convergence</h2>

<p>In order to quantify the convergence, the absolute relative error was
computed. For the 1D signal, the absolute relative errors are shown
below</p>

<p><img src="pics/relerr_1dsignal.png" alt="Relative Error 1D Signal" /></p>

<p>From the plot above, the relative error is seen to converge to around
0.005 at 1000 iterations. The plot of absolute relative error for
different values of Q is shown below</p>

<p><img src="pics/relerr_initial_guess_var.png" alt="Plot of Convergence for different Initial guess
xhat[0]" /></p>

<p>This plot shows that for different values of initial guess, xhat[0],
the absolute relative error is the same after 1 iteration. This suggests
that initial guesses can be arbitrary.</p>

<p>The plot of absolute relative error for different values of Q is shown
below</p>

<p><img src="pics/absrelerr_Qvar.png" alt="Plot of Convergence for different Process Variances
Q" /></p>

<p>This plot shows that the estimated process variance values matter. The
lower the estimated process variance, the better the estimated signal.</p>

<p>The plot for different Measurement Variances R is shown below</p>

<p><img src="pics/absrelerr_R.png" alt="Plot of Convergence for different Process Variances
R" /></p>

<p>This plot shows that the estimated measured variance values matter. The
smaller the estimated process variance, the worse the estimated signal.</p>

<p>The same concept was applied to images. The results for AT3_1m4_01.tif
is shown below for different iterations.</p>

<p><img src="pics/microscope_convergence.png" alt="Plot of Convergence for gaussian noise applied to
AT3_1m4_01.tif" /></p>

<p>Qualitatively, it is seen that as more Kalman Filters are applied, the
image more denoised.</p>

<p>The plot of different noise models and the observed convergence for
absolute relative error is shown below</p>

<p><img src="pics/convergence_noise_models.png" alt="Plot of Convergence for different Noise
Models" /></p>

<p>From the plot, it is seen that the gaussian distribution shows better
convergence then exponential and uniform distributions. This implies
that the noise distribution matters when detecting signals.</p>

<h1 id="conclusion">Conclusion</h1>

<p>From the numerical experiments above, the Kalman Filter serves as a good
denoising filter for 1D signals. The estimated process variance and the
estimated measurement variance affects the convergence but the initial
guess of the signal does not. The lower the estimated process variance,
the better the denoising. The higher the estimated measurement variance,
the better the denoising. The initial guesses display the same
convergence behavior after the 1st iteration. Furthermore, gaussian
noise is suppressed better than exponential and uniform noise
distributions.</p>

<h1 id="discussion">Discussion</h1>

<p>The 1D Kalman Filters can be extended to 2D and higher dimensions to
estimate changing signals. For example, GPS software use the Kalman
Filter to estimate position of a car it is in a tunnel with no GPS
signals. Similarly, accelerometers use Kalman Filters to estimate
velocity and position of an object.</p>

<p>Rejecting outlier measurements is important so that the denoised signal
is not corrupted. This can be done using validation gates.</p>

<p>There are numerical problems with the Kalman Filter that were not
discussed here. Asymmetric covariance matrices present numerical
rounding issues.</p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>I would like to thank Professor Bai for providing me with the
fundamental concepts to undertake this project.</p>

<h1 id="references">References</h1>

<p>[1] “Part III: Low Rank and Compressed Sensing.” Linear Algebra and
Learning from Data, by Gilbert Strang, Wellesley-Cambridge Press, 2019,
pp. 159–200.</p>

<p>[2] Lacey, Tony. “Chapter 11 Tutorial: The Kalman
Filter.” Algorithms Guide,
web.mit.edu/kirtley/kirtley/binlustuff/literature/control/Kalman%20filter.pdf.</p>

<p>[3] Hager, William W. “Updating the Inverse of a Matrix.” SIAM Review,
vol. 31, no. 2, 1989, pp. 221–239., doi:10.1137/1031049.</p>

        
      </section>

      <footer class="page__meta">
        
        


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-06-07T00:00:00-05:00">June 7, 2019</time></p>


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/2019/05/14/Iterative-Subspace-Projection-Methods.html" class="pagination--pager" title="[Project] Iterative Subspace Projection Methods for Large scale Linear Systems and Eigenvalue Problems
">Previous</a>
    
    
      <a href="/2020/08/01/Deep-Diffractive-Neural-Networks.html" class="pagination--pager" title="[Project] Combination Deep Diffractive Neural Networks
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Matt Y. Cheung. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
